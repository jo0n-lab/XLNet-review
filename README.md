# XLNet : Generalized Autoregressive Pretraining for Language Understanding

- [oringinal paper (PDF)](source/XLNet.pdf)
    

## Keywords

`autoencoding (AE)`; `autoregressive (AR)`; `BERT`; `permutation`; `XLNet`; `mask`; `token`; `corrupted sentance`; `restore`; `context representation`; `factorization`; `sequence`; `segment`

## Abstract

- `autoencoding` ë°©ì‹ì˜ BERT ëŠ” masked ëœ í† í°ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.
    
    ![                             masking ë˜ì§€ ì•Šì€ í† í°ë“¤(ì´ˆë¡ ìƒì)ì„ í†µí•´ masked í† í°ì„ ì¶”ë¡ í•œë‹¤. ](source/Untitled%200.png)
    
         masking ë˜ì§€ ì•Šì€ í† í°ë“¤(ì´ˆë¡ ìƒì)ì„ í†µí•´ masked í† í°ì„ ì¶”ë¡ í•œë‹¤. 
    
- í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì `autoregressive` ë°©ì‹ì„ ì±„íƒí•˜ê³ , ëª¨ë“  í† í°ì— ëŒ€í•˜ì—¬ `permutation` í•˜ì—¬ ì´ë¥¼ ê·¹ë³µí•˜ëŠ” `XLNet` ì„ ì†Œê°œí•œë‹¤.
- XLNet ì€ ì„ í–‰ ì—°êµ¬ Transformer-XL ì„ ê³„ìŠ¹í•˜ëŠ” ëª¨ë¸ì´ë‹¤.
- ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ SOTA ë¥¼ ë‹¬ì„±í•˜ê³ , 20ê°œì˜ task ì—ì„œ BERT ë¥¼ ëŠ¥ê°€í•œë‹¤.

## Introduction

- pretrain ì— ë‹¤ì–‘í•œ `objective` ë¥¼ ì ìš©í•˜ì—¬ downstream task ì— finetune ë°©ì‹
    - **Language modeling (GPT)** `autoregressive`
    - **Prefix language modeling**
    - **Masked language modeling (BERT)** `autoencoding`
- `AR objective` language model
    
    ![Untitled](source/Untitled.png)
    
    - density estimation í™œìš©
        - ext corpus í†µí•´ histogramì˜ probability distribution ì„ ì¶”ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ unsupervised model ì— ì í•© (í•˜ë‹¨ ë§í¬ ì°¸ì¡°)
            
            [https://blog.mathpresso.com/mathpresso-ë¨¸ì‹ -ëŸ¬ë‹-ìŠ¤í„°ë””-14-ë°€ë„-ì¶”ì •-density-estimation-38fd7ef729bb](https://blog.mathpresso.com/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-14-%EB%B0%80%EB%8F%84-%EC%B6%94%EC%A0%95-density-estimation-38fd7ef729bb)
            
    - ì™¼ìª½â†’ì˜¤ë¥¸ìª½(ì˜¤ë¥¸ìª½â†’ì™¼ìª½)ì˜ uni-directional ìœ¼ë¡œ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸
    - ìƒˆë¡œìš´ ê¸€ì„ ìƒì„±í•˜ëŠ” task ì— ëŒ€í•´ì„œëŠ” ë§¤ìš° íš¨ê³¼ì 
    - uni-directional í•œ íŠ¹ì„±ìœ¼ë¡œ ê¸€ì˜ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ”(requiring bi-directional) taskì— ëŒ€í•´ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.
    
- `AE objective` based pretraining
    
    ![Untitled](source/Untitled%201.png)
    
    - density estimation ì´ ì•„ë‹Œ corrupted input(ie. masked tokens) ì„ ì›ë¬¸ì¥ ë³µêµ¬ë¥¼ ëª©í‘œ
        - ë”°ë¼ì„œ bi-directional
    - corrupted input êµ¬ì¡°ëŠ” íŠ¹ì • í™•ë¥ ë¡œ ê°€ë ¤ì§„ í† í°ë“¤ì˜ ê´€ê³„ë¥¼ ë¬´ì‹œ
    
    <aside>
    â“ Since density estimation is not part of the objective, BERT is allowed to utilize bidirectional  contexts for reconstruction.
    
    </aside>
    
    - ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‡
        
        <aside>
        â˜ ë°€ë„ ì¶”ì •(í™•ë¥  ë¶„í¬) ë¥¼ objectiveë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ â†’ bidirectional í•˜ê²Œ ë¬¸ì¥ ë³µêµ¬ ê°€ëŠ¥
        
        </aside>
        
        <aside>
        â˜ uni/bi - directional ê³¼ ë°€ë„ ì¶”ì • ì‚¬ì´ì— ê´€ê³„??
        
        </aside>
        
        <aside>
        â˜ unidirectional ê²½ìš°, ì‹œí€€ìŠ¤ ìƒì˜ ëˆ„ì ëœ ì •ë³´ë¥¼ í†µí•´ í™•ë¥  ë¶„í¬(íˆìŠ¤í† ê·¸ë¨) ì¶”ì • ê°€ëŠ¥?
        
        </aside>
        
        <aside>
        â˜ bidirectional ê²½ìš°, ì‹œí€€ìŠ¤ ìƒì—ì„œ ë¬´ì‘ìœ„ë¡œ mask ëœ ë¶€ë¶„(X) ì—ì„œ êµ¬ë© ìƒê¸°ë¯€ë¡œ ì¶”ì • ë¶ˆê°€??
        
        </aside>
        
    - finetuning(real : downstream tasks) ì—ì„œëŠ” mask ë“±ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ pretrain ê³¼ì˜ ê´´ë¦¬ ë°œìƒ
    - `AR` ë°©ì‹ì²˜ëŸ¼ ê²°í•© ë¶„í¬ì—ì„œ product rule ì„ ì ìš©í•˜ì§€ ëª»í•œë‹¤. (â“Â ì— ëŒ€í•œ ë‹µ)
    
    ![                                Aê°€ masked token ì´ë¼ê³  í•œë‹¤ë©´ ìœ„ëŠ” ë‹¨ìˆœíˆ âˆp(A_i) ê°€ ë  ê²ƒì´ë‹¤!](source/Untitled%202.png)
    
        Aê°€ masked token ì´ë¼ê³  í•œë‹¤ë©´ ìœ„ëŠ” ë‹¨ìˆœíˆ âˆp(A_i) ê°€ ë  ê²ƒì´ë‹¤!
    
    - $\mathcal{J}_{BERT}=\log p(New\,|\,is\;a\;city) +\log p(York\,|\,is\;a\;city)$ mask1 ì˜ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ mask2 ì˜ í† í° ê²°ì •ì— í™•ë¥ ì ì¸ ë°˜ì˜ì´ ë˜ì–´ì•¼ í•˜ì§€ë§Œ, ë‹¨ìˆœ ê³±ê³„ì‚°ì„ í†µí•´ ë…ë¦½ì ìœ¼ë¡œ ì·¨ê¸‰ëœë‹¤.
    - ê¸€ì´ ê¸¸ì–´ì§€ëŠ” ìƒí™© [high-level : ë„“ì€ ë²”ìœ„ì˜ ìœ ê¸°ì  ê´€ê³„ê°€ ì¤‘ìš”í•´ì§€ëŠ”] ì„ ì§€ë‚˜ì¹˜ê²Œ ë‹¨ìˆœí™”
    
- ***ìœ„ì˜ ì¥ë‹¨ì ìœ¼ë¡œ `AE` ë°©ì‹ì˜ ë‹¨ì ì€ ì£½ì´ê³  ì¥ì ì„ ìµœëŒ€í•œ ì‚´ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œì„œ `AR` ë°©ì‹ `XLNet` ì œì‹œ!!***
    - `AE +` : ì‹œí€€ìŠ¤ ìƒì˜ token ë“¤ì„ ì„ëŠ”ë‹¤ (`permutation`)
        - ë‹¨ë°©í–¥ì˜ `AR` ë°©ì‹ì—ì„œ ì–‘ë°©í–¥ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë„ë¡
        - í…ìŠ¤íŠ¸ì˜ ë°©í–¥ì„±ì„ ì—†ì—ëŠ” ì‘ì—… (ê¸€ì€ ì™¼â†’ì˜¤ë¥¸ ìª½ìœ¼ë¡œ ì½ìœ¼ë‹ˆê¹)
    - `AE -` : ì‚¬ë¼ì§€ëŠ” í† í°(masked token)ì´ ì—†ìŒ
        - pretrain-finetune ê´´ë¦¬ ì‚¬ë¼ì§
    
- ***ì°¸ì‹ í•œ `AR` objective ë°©ì‹ì˜ ë˜ë‹¤ë¥¸ ì´ì ***
    - Transformer-XL ë°©ì‹ì˜ reccurence mechanism ê³¼ relative encoding scheme ì„ ì°¨ìš©
        
        ![Untitled](source/Untitled%203.png)
        
        - ì‹œí€€ìŠ¤ ë‹¨ìœ„ì˜ í•™ìŠµì€ long term context (real tasks ì—ì„œ ìš”êµ¬ë˜ëŠ”) ì— ëŒ€í•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë¯€ë¡œ ë” í° ë‹¨ìœ„ì˜ segment ë¡œ ë‚˜ëˆ„ì–´ cachingì„ ì§„í–‰í•˜ê³  ë‹¤ìŒ segment ì— ìœ ê¸°ì ìœ¼ë¡œ í•™ìŠµì´ ì „ë‹¬ë˜ë„ë¡
        

## Proposed Method

**Background**

- ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ $\mathtt{x}=[x_1, \cdots , x_n]$ ì— ëŒ€í•˜ì—¬ `AR` ë°©ì‹ê³¼  `BERT` ë¥¼ ë¹„êµí•œë‹¤.
    - ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ forward AR factorization í•™ìŠµ íš¨ìœ¨ì„ ì¦ëŒ€
        - $h_\theta (x_{1\,:\,t-1})$ ì€ ì€ë‹‰ì¸µì˜ context representation, $e(x)$ ëŠ” $x$ ì˜ embedding .
    
    ![Untitled](source/Untitled%204.png)
    
    - BERT ëŠ” ë‹¤ìŒê³¼ ê°™ì´
        - AR ê³¼ ë‹¤ë¥´ê²Œ context ìì²´ê°€ ì•„ë‹Œ ë¬¸ì¥ ë³µêµ¬ì— ê´€ì‹¬ìˆìœ¼ë¯€ë¡œ
        - $\mathtt{x},\hat{x},\bar{x}$ ê°ê° ì…ë ¥ ì‹œí€€ìŠ¤, ë¶•ê´´ëœ $x$(masked), $\hat{x}$ ì˜ masked token
        - $m_t=1\,or\,0,\,T,H_\theta(x)=[H_\theta(x)_1,H_\theta(x)_2,\cdots,H_\theta(x)_T]$ ê°ê° $x_t$ì˜ masked ì—¬ë¶€, ì‹œí€€ìŠ¤ $x$ ì˜ ê¸¸ì´, $x$ ë¥¼ ë§¤í•‘í•´ì£¼ëŠ” transformer
        
        ![Untitled](source/Untitled%205.png)
        

- ìœ„ì˜ ë‘ ë°©ì‹ì˜ ì¥ë‹¨ì 
    - independance assumption :
        - BERT  : ê²°í•©í™•ë¥ ì„ masked token ê°„ì— ë…ë¦½ì´ë¼ê³  ê°€ì •
        - AR : ê²°í•©í™•ë¥ ì„ chain rule ì„ ì ìš©í•˜ì—¬ ì¶”ì • (bayesian chain rule)
    - input noise
        - BERT : real task ì—ì„œëŠ” ë“±ì¥í•˜ì§€ ì•ŠëŠ” pretrain ìƒì˜ mask
        - AR : ì™„ì „í•œ ë¬¸ì¥
    - context dependency
        - BERT : $p_\theta(x_t|\bar{x})$ ë¡œ bidirectional
        - AR : $p_\theta(x_t|x_{<t})$$p_\theta(x_t|\bar{x})$  ë¡œ unidirectional

**Objective : Permutation Language Modeling (Permutation AR)**

- ê¸¸ì´ T ì˜ ì…ë ¥ ì‹œí€€ìŠ¤ $\mathtt{x}=[x_1,\cdots,x_T]$ $x$ëŠ” í† í°
    - permutation ì§„í–‰í•˜ë¯€ë¡œ T! ê°€ì§€ì˜ ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ë°œìƒ
    - ëª¨ë¸ì€ ì–‘ë°©í–¥ìœ¼ë¡œ í† í°ì˜ ìœ„ì¹˜ì— ëŒ€í•œ í•™ìŠµì„ ë” íš¨ê³¼ì ìœ¼ë¡œ
- ë°œìƒí•œ ì‹œí€€ìŠ¤ì— ëŒ€í•œ set $Z_T$ ì™€ ê°€ëŠ¥í•œ ëª¨ë“  ê¸¸ì´ T $(1,2,\cdots,T)$
    - $z_t,\mathtt{z}_{t<}$ ê°ê° $Z_T$ ì˜ të²ˆì§¸ ì›ì†Œ ì‹œí€€ìŠ¤,  ì•ì˜ t-1 í† í°ë“¤ì— ëŒ€í•œ permutation ì‹œí€€ìŠ¤
    - $x_t$ ë¬´ì‘ìœ„ í† í° (ë‹¨ $x_i \neq x_t$ )
    - ì•„ë˜ ì‹ì„ í†µí•´ bidirectional íš¨ê³¼ë¥¼ ì–»ëŠ”ë‹¤.
    
    ![Untitled](source/Untitled%206.png)
    

**Architecture : Two-Stream Self-Attention for Target-Aware Representations**

- language modeling objective(ë‹¤ìŒ í† í°ì„ ì¶”ë¡ ) íŠ¹ì„±
    - ìˆœìˆ˜í•œ ê¸°ëŠ¥ìœ¼ë¡œì˜ Transformer parameterization ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
        - Softmax ë¡œ paremeter â€¦ ì–´ì©Œêµ¬ ì €ì©Œêµ¬..
        - ì‹ì„ ë³´ë©´, $h_\theta(\mathtt{x}_{\mathtt{z}_{<t}})$ ($\mathtt{z}_{<t}$: ì•ì˜ t-1 í† í°ì— ëŒ€í•œ permutation ì‹œí€€ìŠ¤ ì§‘í•© / $\mathtt{x}_{\mathtt{z}{<t}}$ :  t-1 í† í° ë“¤ì˜ ì§‘í•© /  ê°ê° ìœ„ì¹˜ ì •ë³´ëŠ” ê³ ì •) ê°€ $z_t$( $\mathtt{z}_{<t}$ì˜ ì›ì†Œ ì‹œí€€ìŠ¤(ìœ„ì¹˜ ì •ë³´)) ì™€ ê´€ë ¨ì´ ì—†ë‹¤.
            
            ![                                 z_t ì— ëŒ€í•œ í•­ì„ ëˆˆì„ ì”¼ê³ !! ì°¾ì•„ë´ë„ ì°¾ì„ ìˆ˜ ì—†ë”°ğŸ”¥](source/Untitled%2011.png)
            
                $z_t$ ì— ëŒ€í•œ í•­ì„ ëˆˆì„ ì”¼ê³ !! ì°¾ì•„ë´ë„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ã… ğŸ”¥
    
               


       
        > **Note ğŸ”¥**
        >
        > ì¦‰ $z_t$ ê°€ ì•„ë‹Œ $[z_t,z_{t+1},\cdots,z_{t+k},\cdots,z_{T}]$ ì˜ ì•„ë¬´ z ì— ëŒ€í•œ ì¶”ë¡ ì¼ ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒ!!
        



- ì´ëŸ¬í•œ ì• ë§¤í•¨ì„ í•´ê²°í•˜ê¸° ìœ„í•´ â€˜stand target positionâ€™
    - $h_\theta$ ë¥¼ $g_\theta$ ë¡œ ëŒ€ì²´ $g_\theta(\mathtt{x}_{\mathtt{z}_{<t}},z_t)$
        - ì›ì†Œ ì‹œí€€ìŠ¤ $z_t$ ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆì–´ì„œ attention í†µí•´ $\mathtt{x}_{\mathtt{z}_{<t}}$ì— ëŒ€í•œ ë¬¸ë§¥ ì¶”ë¡  íšë“
        - ê·¸ëŸ¬ë‚˜ ë¬¸ì œ!
            - $x_{z_t},g_\theta(\mathtt{x}_{\mathtt{z}_{<t}},z_t)$ í† í°ì„ ì¶”ë¡ í•˜ê¸° ìœ„í•´ì„  $z_t$ ë§Œ ì‚¬ìš©í•´ì•¼ í•˜ê³ , $x_{z_t}$ëŠ” ì‚¬ìš©í•˜ë©´ ì•ˆëŒ!
                - ì¶”ë¡ í•˜ê³ ì í•˜ëŠ” ê°’ì´ ì´ë¯¸ ì£¼ì–´ì§€ë©´ ì˜ë¯¸ê°€ ì—†ìŒ (ë„ˆë¬´ë‚˜ trivial(ìëª…)í•´ì„œ)
            - $x_{z_j}$ ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ $j>t,g_\theta(\mathtt{x}_{\mathtt{z}_{<t}},z_t)$ ì—ì„œë„ $x_t$ ê°€ ì°¸ì¡°ëœë‹¤.
            
- ë˜ ì´ê±¸ ë™ì‹œì— í•´ê²°í•˜ëŠ” â€˜two-streamâ€™
    - content representation  : $h_\theta(\mathtt{x}_{\mathtt{z}_{\leq t}},z_t)\;\;or\;\;h_{z_t}$
        - Transformer ì˜ ì€ë‹‰ representation ê³¼ ë¹„ìŠ·, ë¬¸ë§¥($\mathtt{x}_{\mathtt{z}_{<t}}$) ê³¼ $x_{z_t}$ ëª¨ë‘ ì¸ì½”ë”©
    - query representation : $g_\theta(\mathtt{x}_{\mathtt{z}_{< t}},z_t)\;\;or\;\;g_{z_t}$
        - ë¬¸ë§¥($\mathtt{x}_{\mathtt{z}_{<t}}$) ê³¼ ìœ„ì¹˜($z_t$) ë§Œ ì¶”ë¡ 
        - $x_{z_t}$ì˜ ë‚´ìš© ìì²´ëŠ” x ì°¸ì¡°

![                     Q,K,V ë¥¼ ê°ê° ë”°ë¡œ ë¶„ë¦¬í•´ì„œ ë‘ê°œì˜ íë¦„(two-stream) architecture êµ¬í˜„](source/Untitled%207.png)

     Q,K,V ë¥¼ ê°ê° ë”°ë¡œ ë¶„ë¦¬í•´ì„œ ë‘ê°œì˜ íë¦„(two-stream) architecture êµ¬í˜„

- partial prediction
    - ìš°ìˆ˜í•œ ì ë“¤ì´ ë§ì§€ë§Œ
        - permutation ë”°ë¥¸ ìµœì í™” ë¬¸ì œ
        - ì ê·¼ ì†ë„(slow convergence) (í•™ìŠµì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” ê±´ê°€?)
    - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
        - ë¬´ì‘ìœ„ ë°°ì—´ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ í† í°ë§Œ ì¶”ë¡  (the firstâ†’ the last $T)$
            - c ë¥¼ ë„ì…í•˜ì—¬ (non-target,target) ($\mathtt{z}_{\leq c},\mathtt{z}_{>c})$ ìœ¼ë¡œ ìŠ¬ë¼ì´ìŠ¤
                
                ![                              ë„“ì€ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ë° ìˆì–´ z>c ê°€ ë” íš¨ê³¼ì ì´ë‹ˆê¹ target](source/Untitled%208.png)
                
                                              ë„“ì€ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ë° ìˆì–´ z>c ê°€ ë” íš¨ê³¼ì ì´ë‹ˆê¹ target
                
            
            ![Untitled](source/Untitled%209.png)
            

**Incorporating Ideas from Transformer-XL**

- XL-Transformer ì˜ relative positional encoding & segment recurrence
    - $\tilde{\mathtt{x}}=\mathtt{s}_{1:T},\mathtt{x}=\mathtt{s}_{T+1:2T}$.    $[1,\cdots,T],[T+1,\cdots,2T]$
    - ~ í•­ë“¤($h_{z_t}^{(m-1)}$ì˜ representation)ì— ì˜í•´ caching(update) ì´ ì§€ì†ì ìœ¼ë¡œ ì´ë¤„ì§€ë¯€ë¡œ ë” ìœ ê¸°ì ì¸ architecture â†’ long term context ì— ë” íš¨ê³¼ì ìœ¼ë¡œ
    
    ![                                                      KVì— ë‘ê°œ í•­ì´ ë“¤ì–´ê°€ëŠ” ê²ƒì„ í™•ì¸!](source/Untitled%2010.png)
    
        KVì— ë‘ê°œ í•­ì´ ë“¤ì–´ê°€ëŠ” ê²ƒì„ í™•ì¸!
    
    ![Untitled](source/Untitled%203.png)
    

**Modeling Multiple Segments**

- ì´ì œ ìœ„ì˜ ê²ƒë“¤ì„ ì´ìš©í•´ì„œ ì–´ë–»ê²Œ pretrain í•  ê²ƒì´ëƒ
    - ë¬´ì‘ìœ„ segment ë‘ê°œë¥¼ ê³¨ë¼ concat í›„ í•˜ë‚˜ì˜ sequenceë¡œ ì·¨ê¸‰ â†’ permuation
    - ê°™ì€ context ìƒì˜ memory ë§Œ ì‚¬ìš© (ìœ„ì—ì„œ ì–¸ê¸‰í•œ caching ê³¼ ê´€ë ¨í•œ ë©”ëª¨ë¦¬)
    - BERT ì˜ [CLS, A, SEP, B, SEP] ì„ ê·¸ëŒ€ë¡œ inputì— ì‚¬ìš©
        - CLS : sequence ì‹œì‘, A : ì²«ë²ˆì§¸ segment, SEP : ë¬¸ì¥ ë, B : ë‘ë²ˆì§¸ segment ì‹œì‘
        - (XLNet -Large ê²½ìš° NSP(Next Sentence Prediction)ëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤.